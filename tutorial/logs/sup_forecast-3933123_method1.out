
Lmod is automatically replacing "gcc-native/14.2" with "gcc/12.2.0".


The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Job configuration:
  ERA5_DIR      = /lustre/orion/lrn036/world-shared/ERA5_npz/1.40625_deg/
  FORECAST_TYPE = direct
  MODEL         = res_slimvit
  PRED_RANGE    = 120
  MAX_EPOCHS    = 50
  PATIENCE      = 20
  OUTPUT_DIR    = /lustre/orion/csc662/proj-shared/janet/forecasting
  CHECKPOINT    = 
world_size 8 num_nodes 1
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading model: res_slimvit
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading optimizer adamw
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
Loading training loss: lat_mse
No train transform
No train transform
No train transform
No train transform
No train transform
No train transform
No train transform
No train transform
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_rmse
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_acc
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation transform: denormalize
No validation transform
Loading validation transform: denormalize
No validation transform
No validation transform
Loading validation loss: lat_mse
Loading validation loss: lat_mse
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation loss: lat_mse
No validation transform
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation transform: denormalize
Loading validation transform: denormalize
No validation transform
No validation transform
No validation transform
Loading validation transform: denormalize
No validation transform
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_rmse
Loading test loss: lat_acc
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
Loading test transform: denormalize
Loading test loss: lat_acc
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
Loading test transform: denormalize
Loading test transform: denormalize
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
Loading test transform: denormalize
Loading test transform: denormalize
[rank: 6] Seed set to 0
[rank: 0] Seed set to 0
[rank: 1] Seed set to 0
[rank: 2] Seed set to 0
[rank: 3] Seed set to 0
[rank: 4] Seed set to 0
[rank: 5] Seed set to 0
[rank: 7] Seed set to 0
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/lightning_fabric/strategies/fsdp.py:697: `FSDPStrategy(activation_checkpointing=<class 'timm.models.vision_transformer.Block'>)` is deprecated, use `FSDPStrategy(activation_checkpointing_policy={<class 'timm.models.vision_transformer.Block'>})` instead.
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
You are using a CUDA device ('AMD Instinct MI250X') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
>> Resuming from: /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
[W1129 23:11:16.238973030 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.238975484 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.238985113 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.238983940 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.239000652 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.238996585 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:16.238992387 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W1129 23:11:17.364929585 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09069.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
┏━━━┳━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃   ┃ Name ┃ Type         ┃ Params ┃ Mode  ┃
┡━━━╇━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0 │ net  │ Res_Slim_ViT │  6.5 M │ train │
└───┴──────┴──────────────┴────────┴───────┘
Trainable params: 6.5 M                                                         
Non-trainable params: 0                                                         
Total params: 6.5 M                                                             
Total estimated model params size (MB): 25                                      
Modules in train mode: 396                                                      
Modules in eval mode: 0                                                         
Restoring states from the checkpoint path at /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
Restored all states from the checkpoint at /lustre/orion/csc662/proj-shared/janet/forecasting/v4_baseline_test_high_ressolution_res_slimvit_direct_forecasting_120/checkpoints/last.ckpt
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(

Training: |          | 0/? [00:00<?, ?it/s]
Training: |          | 0/? [00:00<?, ?it/s]
Epoch 3: |          | 0/? [00:00<?, ?it/s] 
Epoch 3: |          | 20/? [02:21<00:00,  0.14it/s]
Epoch 3: |          | 20/? [02:22<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.0793]
Epoch 3: |          | 40/? [03:50<00:00,  0.17it/s, v_num=5, train/lat_mse:aggregate=0.0793]
Epoch 3: |          | 40/? [03:51<00:00,  0.17it/s, v_num=5, train/lat_mse:aggregate=0.0585]
Epoch 3: |          | 40/? [04:52<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.056] 
Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.056]         
Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.056]
Epoch 4: |          | 20/? [03:22<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.056]
Epoch 4: |          | 20/? [03:23<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.0556]
Epoch 4: |          | 40/? [04:51<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.0556]
Epoch 4: |          | 40/? [04:52<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.0452]
Epoch 4: |          | 40/? [05:42<00:00,  0.12it/s, v_num=5, train/lat_mse:aggregate=0.0413]
Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0413]         
Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0413]
Epoch 5: |          | 20/? [03:32<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0413]
Epoch 5: |          | 20/? [03:33<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0369]
Epoch 5: |          | 40/? [05:01<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0369]
Epoch 5: |          | 40/? [05:02<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0343]
Epoch 5: |          | 40/? [05:51<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0338]
Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0338]         
Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0338]
Epoch 6: |          | 20/? [03:25<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.0338]
Epoch 6: |          | 20/? [03:26<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.0313]
Epoch 6: |          | 40/? [04:54<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.0313]
Epoch 6: |          | 40/? [04:55<00:00,  0.14it/s, v_num=5, train/lat_mse:aggregate=0.0312]
Epoch 6: |          | 40/? [05:44<00:00,  0.12it/s, v_num=5, train/lat_mse:aggregate=0.0309]
Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0309]         
Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0309]
Epoch 7: |          | 20/? [03:31<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0309]
Epoch 7: |          | 20/? [03:32<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0292]
Epoch 7: |          | 40/? [05:01<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0292]
Epoch 7: |          | 40/? [05:01<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0335]
Epoch 7: |          | 40/? [05:51<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0306]
Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0306]         
Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0306]
Epoch 8: |          | 20/? [03:29<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.0306]
Epoch 8: |          | 20/? [03:30<00:00,  0.10it/s, v_num=5, train/lat_mse:aggregate=0.0264]
Epoch 8: |          | 40/? [04:58<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0264]
Epoch 8: |          | 40/? [04:59<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0268]
Epoch 8: |          | 40/? [05:51<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.028] 
Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.028]         
Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.028]
Epoch 9: |          | 20/? [03:34<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.028]
Epoch 9: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0272]
Epoch 9: |          | 40/? [05:03<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0272]
Epoch 9: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0252]
Epoch 9: |          | 40/? [05:53<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0248]
Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0248]         
Epoch 10: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0248]
Epoch 10: |          | 20/? [03:34<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0248]
Epoch 10: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0253]
Epoch 10: |          | 40/? [05:05<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0253]
Epoch 10: |          | 40/? [05:06<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0255]
Epoch 10: |          | 40/? [05:55<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0258]
Epoch 10: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0258]         
Epoch 11: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0258]
Epoch 11: |          | 20/? [03:33<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0258]
Epoch 11: |          | 20/? [03:34<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0256]
Epoch 11: |          | 40/? [05:02<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0256]
Epoch 11: |          | 40/? [05:03<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.024] 
Epoch 11: |          | 40/? [05:52<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.023]
Epoch 11: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.023]         
Epoch 12: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.023]
Epoch 12: |          | 20/? [03:33<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.023]
Epoch 12: |          | 20/? [03:33<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0245]
Epoch 12: |          | 40/? [05:02<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0245]
Epoch 12: |          | 40/? [05:02<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0231]
Epoch 12: |          | 40/? [05:52<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0237]
Epoch 12: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0237]         
Epoch 13: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0237]
Epoch 13: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0237]
Epoch 13: |          | 20/? [03:36<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0224]
Epoch 13: |          | 40/? [05:05<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0224]
Epoch 13: |          | 40/? [05:05<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0224]
Epoch 13: |          | 40/? [05:55<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.023] 
Epoch 13: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.023]         
Epoch 14: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.023]
Epoch 14: |          | 20/? [03:38<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.023]
Epoch 14: |          | 20/? [03:39<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0231]
Epoch 14: |          | 40/? [05:08<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0231]
Epoch 14: |          | 40/? [05:08<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0218]
Epoch 14: |          | 40/? [05:58<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0212]
Epoch 14: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0212]         
Epoch 15: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0212]
Epoch 15: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0212]
Epoch 15: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0211]
Epoch 15: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0211]
Epoch 15: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0255]
Epoch 15: |          | 40/? [05:54<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0246]
Epoch 15: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0246]         
Epoch 16: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0246]
Epoch 16: |          | 20/? [03:36<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0246]
Epoch 16: |          | 20/? [03:37<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0225]
Epoch 16: |          | 40/? [05:06<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0225]
Epoch 16: |          | 40/? [05:07<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0226]
Epoch 16: |          | 40/? [05:56<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0215]
Epoch 16: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0215]         
Epoch 17: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0215]
Epoch 17: |          | 20/? [03:34<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0215]
Epoch 17: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0203]
Epoch 17: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0203]
Epoch 17: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0219]
Epoch 17: |          | 40/? [05:57<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0241]
Epoch 17: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0241]         
Epoch 18: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0241]
Epoch 18: |          | 20/? [03:44<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0241]
Epoch 18: |          | 20/? [03:45<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0196]
Epoch 18: |          | 40/? [05:14<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0196]
Epoch 18: |          | 40/? [05:14<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.018] 
Epoch 18: |          | 40/? [06:04<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.0176]
EpocTime limit reached. Elapsed time is 1:56:14. Signaling Trainer to stop.
h 18: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0176]         
Epoch 19: |          | 0/? [00:00<?, ?it/s, v_num=5, train/lat_mse:aggregate=0.0176]
Epoch 19: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0176]
Epoch 19: |          | 20/? [03:35<00:00,  0.09it/s, v_num=5, train/lat_mse:aggregate=0.0195]
Epoch 19: |          | 40/? [05:04<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0195]
Epoch 19: |          | 40/? [05:05<00:00,  0.13it/s, v_num=5, train/lat_mse:aggregate=0.0228]
Epoch 19: |          | 40/? [05:54<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.021] 
Epoch 19: |          | 40/? [05:56<00:00,  0.11it/s, v_num=5, train/lat_mse:aggregate=0.021]
Epoch 20: |          | 0/? [00:00<?, ?it/s][Step 1121] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 20: |          | 20/? [02:23<00:00,  0.14it/s]
Epoch 20: |          | 20/? [02:24<00:00,  0.14it/s, v_num=6, train/lat_mse:aggregate=0.0197]
Epoch 20: |          | 40/? [03:53<00:00,  0.17it/s, v_num=6, train/lat_mse:aggregate=0.0197]
Epoch 20: |          | 40/? [03:53<00:00,  0.17it/s, v_num=6, train/lat_mse:aggregate=0.0189][Step 1171] GPU Memory - Allocated: 0.17GB, Reserved: 6.48GB, Peak: 15.55GB

Epoch 20: |          | 40/? [04:53<00:00,  0.14it/s, v_num=6, train/lat_mse:aggregate=0.0185]
Epoch 20: |          | 40/? [04:55<00:00,  0.14it/s, v_num=6, train/lat_mse:aggregate=0.0185]
Epoch 21: |          | 0/? [00:00<?, ?it/s][Step 1177] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 21: |          | 20/? [02:15<00:00,  0.15it/s]
Epoch 21: |          | 20/? [02:16<00:00,  0.15it/s, v_num=7, train/lat_mse:aggregate=0.0195]
Epoch 21: |          | 40/? [03:45<00:00,  0.18it/s, v_num=7, train/lat_mse:aggregate=0.0195]
Epoch 21: |          | 40/? [03:46<00:00,  0.18it/s, v_num=7, train/lat_mse:aggregate=0.0187][Step 1227] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB
Epoch 21: |          | 40/? [04:46<00:00,  0.14it/s, v_num=7, train/lat_mse:aggregate=0.0182]
Epoch 21: |          | 40/? [04:47<00:00,  0.14it/s, v_num=7, train/lat_mse:aggregate=0.0182]
Epoch 22: |          | 0/? [00:00<?, ?it/s][Step 1233] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 22: |          | 20/? [02:21<00:00,  0.14it/s]
Epoch 22: |          | 20/? [02:22<00:00,  0.14it/s, v_num=8, train/lat_mse:aggregate=0.019]
Epoch 22: |          | 40/? [03:49<00:00,  0.17it/s, v_num=8, train/lat_mse:aggregate=0.019]
Epoch 22: |          | 40/? [03:50<00:00,  0.17it/s, v_num=8, train/lat_mse:aggregate=0.0185][Step 1283] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 22: |          | 40/? [04:50<00:00,  0.14it/s, v_num=8, train/lat_mse:aggregate=0.0181]
Epoch 22: |          | 40/? [04:51<00:00,  0.14it/s, v_num=8, train/lat_mse:aggregate=0.0181]
Epoch 23: |          | 0/? [00:00<?, ?it/s][Step 1289] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 23: |          | 20/? [02:27<00:00,  0.14it/s]
Epoch 23: |          | 20/? [02:27<00:00,  0.14it/s, v_num=9, train/lat_mse:aggregate=0.0188]
Epoch 23: |          | 40/? [03:54<00:00,  0.17it/s, v_num=9, train/lat_mse:aggregate=0.0188]
Epoch 23: |          | 40/? [03:55<00:00,  0.17it/s, v_num=9, train/lat_mse:aggregate=0.018] [Step 1339] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 23: |          | 40/? [04:54<00:00,  0.14it/s, v_num=9, train/lat_mse:aggregate=0.0179]
Epoch 23: |          | 40/? [04:56<00:00,  0.14it/s, v_num=9, train/lat_mse:aggregate=0.0179]
Epoch 24: |          | 0/? [00:00<?, ?it/s][Step 1345] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 24: |          | 20/? [02:27<00:00,  0.14it/s]
Epoch 24: |          | 20/? [02:28<00:00,  0.13it/s, v_num=10, train/lat_mse:aggregate=0.0189]
Epoch 24: |          | 40/? [03:56<00:00,  0.17it/s, v_num=10, train/lat_mse:aggregate=0.0189]
Epoch 24: |          | 40/? [03:57<00:00,  0.17it/s, v_num=10, train/lat_mse:aggregate=0.0183][Step 1395] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 24: |          | 40/? [04:57<00:00,  0.13it/s, v_num=10, train/lat_mse:aggregate=0.0177]
Epoch 24: |          | 40/? [04:58<00:00,  0.13it/s, v_num=10, train/lat_mse:aggregate=0.0177]
Epoch 25: |          | 0/? [00:00<?, ?it/s][Step 1401] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 25: |          | 20/? [02:27<00:00,  0.14it/s]
Epoch 25: |          | 20/? [02:28<00:00,  0.13it/s, v_num=11, train/lat_mse:aggregate=0.0184]
Epoch 25: |          | 40/? [03:57<00:00,  0.17it/s, v_num=11, train/lat_mse:aggregate=0.0184]
Epoch 25: |          | 40/? [03:58<00:00,  0.17it/s, v_num=11, train/lat_mse:aggregate=0.0177][Step 1451] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 25: |          | 40/? [04:59<00:00,  0.13it/s, v_num=11, train/lat_mse:aggregate=0.0174]
Epoch 25: |          | 40/? [05:00<00:00,  0.13it/s, v_num=11, train/lat_mse:aggregate=0.0174]
Epoch 26: |          | 0/? [00:00<?, ?it/s][Step 1457] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 26: |          | 20/? [02:21<00:00,  0.14it/s]
Epoch 26: |          | 20/? [02:21<00:00,  0.14it/s, v_num=12, train/lat_mse:aggregate=0.0181]
Epoch 26: |          | 40/? [03:49<00:00,  0.17it/s, v_num=12, train/lat_mse:aggregate=0.0181]
Epoch 26: |          | 40/? [03:50<00:00,  0.17it/s, v_num=12, train/lat_mse:aggregate=0.0175][Step 1507] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 26: |          | 40/? [04:50<00:00,  0.14it/s, v_num=12, train/lat_mse:aggregate=0.0174]
Epoch 26: |          | 40/? [04:51<00:00,  0.14it/s, v_num=12, train/lat_mse:aggregate=0.0174]
Epoch 27: |          | 0/? [00:00<?, ?it/s][Step 1513] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 27: |          | 20/? [02:38<00:00,  0.13it/s]
Epoch 27: |          | 20/? [02:38<00:00,  0.13it/s, v_num=13, train/lat_mse:aggregate=0.018]
Epoch 27: |          | 40/? [04:06<00:00,  0.16it/s, v_num=13, train/lat_mse:aggregate=0.018]
Epoch 27: |          | 40/? [04:07<00:00,  0.16it/s, v_num=13, train/lat_mse:aggregate=0.0176][Step 1563] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB
Epoch 27: |          | 40/? [05:05<00:00,  0.13it/s, v_num=13, train/lat_mse:aggregate=0.0173]
Epoch 27: |          | 40/? [05:07<00:00,  0.13it/s, v_num=13, train/lat_mse:aggregate=0.0173]
Epoch 28: |          | 0/? [00:00<?, ?it/s][Step 1569] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 28: |          | 20/? [02:18<00:00,  0.14it/s]
Epoch 28: |          | 20/? [02:18<00:00,  0.14it/s, v_num=14, train/lat_mse:aggregate=0.0177]
Epoch 28: |          | 40/? [03:47<00:00,  0.18it/s, v_num=14, train/lat_mse:aggregate=0.0177]
Epoch 28: |          | 40/? [03:47<00:00,  0.18it/s, v_num=14, train/lat_mse:aggregate=0.0172][Step 1619] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 28: |          | 40/? [04:46<00:00,  0.14it/s, v_num=14, train/lat_mse:aggregate=0.0168]
Epoch 28: |          | 40/? [04:48<00:00,  0.14it/s, v_num=14, train/lat_mse:aggregate=0.0168]
Epoch 29: |          | 0/? [00:00<?, ?it/s][Step 1625] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 29: |          | 20/? [02:22<00:00,  0.14it/s]
Epoch 29: |          | 20/? [02:23<00:00,  0.14it/s, v_num=15, train/lat_mse:aggregate=0.0178]
Epoch 29: |          | 40/? [03:52<00:00,  0.17it/s, v_num=15, train/lat_mse:aggregate=0.0178]
Epoch 29: |          | 40/? [03:53<00:00,  0.17it/s, v_num=15, train/lat_mse:aggregate=0.017] [Step 1675] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB
Epoch 29: |          | 40/? [04:51<00:00,  0.14it/s, v_num=15, train/lat_mse:aggregate=0.0165]
Epoch 29: |          | 40/? [04:53<00:00,  0.14it/s, v_num=15, train/lat_mse:aggregate=0.0165]
Epoch 30: |          | 0/? [00:00<?, ?it/s][Step 1681] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 30: |          | 20/? [02:19<00:00,  0.14it/s]
Epoch 30: |          | 20/? [02:20<00:00,  0.14it/s, v_num=16, train/lat_mse:aggregate=0.0175]
Epoch 30: |          | 40/? [03:49<00:00,  0.17it/s, v_num=16, train/lat_mse:aggregate=0.0175]
Epoch 30: |          | 40/? [03:49<00:00,  0.17it/s, v_num=16, train/lat_mse:aggregate=0.017] [Step 1731] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 30: |          | 40/? [04:49<00:00,  0.14it/s, v_num=16, train/lat_mse:aggregate=0.0164]
Epoch 30: |          | 40/? [04:50<00:00,  0.14it/s, v_num=16, train/lat_mse:aggregate=0.0164]
Epoch 31: |          | 0/? [00:00<?, ?it/s][Step 1737] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 31: |          | 20/? [02:16<00:00,  0.15it/s]
Epoch 31: |          | 20/? [02:17<00:00,  0.15it/s, v_num=17, train/lat_mse:aggregate=0.0174]
Epoch 31: |          | 40/? [03:45<00:00,  0.18it/s, v_num=17, train/lat_mse:aggregate=0.0174]
Epoch 31: |          | 40/? [03:45<00:00,  0.18it/s, v_num=17, train/lat_mse:aggregate=0.0169][Step 1787] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB

Epoch 31: |          | 40/? [04:44<00:00,  0.14it/s, v_num=17, train/lat_mse:aggregate=0.0163]
Epoch 31: |          | 40/? [04:45<00:00,  0.14it/s, v_num=17, train/lat_mse:aggregate=0.0163]
Epoch 32: |          | 0/? [00:00<?, ?it/s][Step 1793] GPU Memory - Allocated: 0.29GB, Reserved: 0.72GB, Peak: 15.55GB

Epoch 32: |          | 20/? [02:18<00:00,  0.14it/s]
Epoch 32: |          | 20/? [02:19<00:00,  0.14it/s, v_num=18, train/lat_mse:aggregate=0.0172]
Epoch 32: |          | 40/? [03:47<00:00,  0.18it/s, v_num=18, train/lat_mse:aggregate=0.0172]
Epoch 32: |          | 40/? [03:48<00:00,  0.18it/s, v_num=18, train/lat_mse:aggregate=0.0164][Step 1843] GPU Memory - Allocated: 0.17GB, Reserved: 6.49GB, Peak: 15.55GB
Epoch 32: |          | 40/? [04:47<00:00,  0.14it/s, v_num=18, train/lat_mse:aggregate=0.0161]
Epoch 32: |          | 40/? [04:48<00:00,  0.14it/s, v_num=18, train/lat_mse:aggregate=0.0161]


LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/pytorch_lightning/strategies/fsdp.py:304: A FSDP `auto_wrap_policy` is set, but the model is already wrapped. The policy will be ignored.
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/lightning_fabric/strategies/fsdp.py:726: FSDP checkpointing is configured, but the model already contains checkpointed layers. Checkpointing will be ignored.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.

Testing: |          | 0/? [00:00<?, ?it/s]
Testing: |          | 0/? [00:00<?, ?it/s]

real	102m7.266s
user	0m0.020s
sys	0m0.034s
