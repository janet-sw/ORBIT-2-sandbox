
Lmod is automatically replacing "gcc-native/14.2" with "gcc/12.2.0".


The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Job configuration:
  ERA5_DIR      = /lustre/orion/lrn036/world-shared/ERA5_npz/1.40625_deg/
  FORECAST_TYPE = direct
  MODEL         = res_slimvit
  PRED_RANGE    = 120
  MAX_EPOCHS    = 50
  PATIENCE      = 10
  OUTPUT_DIR    = /lustre/orion/csc662/proj-shared/janet/forecasting
  CHECKPOINT    = 
world_size 1 num_nodes 0
Loading model: res_slimvit
Loading optimizer adamw
Loading learning rate scheduler: linear-warmup-cosine-annealing
Loading training loss: lat_mse
No train transform
Loading validation loss: lat_rmse
Loading validation loss: lat_acc
Loading validation loss: lat_mse
Loading validation transform: denormalize
Loading validation transform: denormalize
No validation transform
Loading test loss: lat_rmse
Loading test loss: lat_acc
Loading test transform: denormalize
Loading test transform: denormalize
[rank: 0] Seed set to 0
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/lightning_fabric/strategies/fsdp.py:697: `FSDPStrategy(activation_checkpointing=<class 'timm.models.vision_transformer.Block'>)` is deprecated, use `FSDPStrategy(activation_checkpointing_policy={<class 'timm.models.vision_transformer.Block'>})` instead.
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
>> Fresh run
You are using a CUDA device ('AMD Instinct MI250X') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
[W1115 17:03:01.955349886 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [frontier09077.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/distributed/fsdp/_init_utils.py:430: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision bf16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
┏━━━┳━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓
┃   ┃ Name ┃ Type         ┃ Params ┃ Mode  ┃
┡━━━╇━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩
│ 0 │ net  │ Res_Slim_ViT │  2.9 M │ train │
└───┴──────┴──────────────┴────────┴───────┘
Trainable params: 2.9 M                                                         
Non-trainable params: 0                                                         
Total params: 2.9 M                                                             
Total estimated model params size (MB): 11                                      
Modules in train mode: 208                                                      
Modules in eval mode: 0                                                         
SLURM auto-requeueing enabled. Setting signal handlers.
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 20/? [03:36<00:00,  0.09it/s]Epoch 0: |          | 20/? [03:36<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.932]Epoch 0: |          | 40/? [07:15<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.932]Epoch 0: |          | 40/? [07:16<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.922]Epoch 0: |          | 60/? [10:49<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.922]Epoch 0: |          | 60/? [10:49<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.935]Epoch 0: |          | 80/? [14:59<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.935]Epoch 0: |          | 80/? [14:59<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.932]Epoch 0: |          | 100/? [19:13<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.932]Epoch 0: |          | 100/? [19:13<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.920]Epoch 0: |          | 120/? [23:29<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.920]Epoch 0: |          | 120/? [23:29<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.920]Epoch 0: |          | 140/? [27:54<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.920]Epoch 0: |          | 140/? [27:54<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.921]Epoch 0: |          | 160/? [32:08<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.921]Epoch 0: |          | 160/? [32:08<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.918]Epoch 0: |          | 180/? [36:29<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.918]Epoch 0: |          | 180/? [36:29<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.929]Epoch 0: |          | 200/? [40:53<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.929]Epoch 0: |          | 200/? [40:53<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.930]Epoch 0: |          | 220/? [45:13<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.930]Epoch 0: |          | 220/? [45:13<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.925]Epoch 0: |          | 240/? [49:31<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.925]Epoch 0: |          | 240/? [49:31<00:00,  0.08it/s, v_num=9, train/lat_mse:aggregate=0.929]Epoch 0: |          | 260/? [50:22<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.929]Epoch 0: |          | 260/? [50:22<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.918]Epoch 0: |          | 280/? [50:37<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.918]Epoch 0: |          | 280/? [50:37<00:00,  0.09it/s, v_num=9, train/lat_mse:aggregate=0.927]Epoch 0: |          | 300/? [50:52<00:00,  0.10it/s, v_num=9, train/lat_mse:aggregate=0.927]Epoch 0: |          | 300/? [50:53<00:00,  0.10it/s, v_num=9, train/lat_mse:aggregate=0.924]Epoch 0: |          | 320/? [51:10<00:00,  0.10it/s, v_num=9, train/lat_mse:aggregate=0.924]Epoch 0: |          | 320/? [51:10<00:00,  0.10it/s, v_num=9, train/lat_mse:aggregate=0.931]Epoch 0: | /lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:763: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict will be returned.
  warnings.warn(
/lustre/orion/csc662/proj-shared/janet/miniconda3/envs/orbit/lib/python3.13/site-packages/torch/distributed/fsdp/_state_dict_utils.py:701: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict will be returned.
  warnings.warn(
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 3895460.0 ON frontier09077 CANCELLED AT 2025-11-15T19:02:41 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 3895460 ON frontier09077 CANCELLED AT 2025-11-15T19:02:41 DUE TO TIME LIMIT ***
